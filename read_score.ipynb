{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c686b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for final_score.csv files...\n",
      "==================================================\n",
      "✓ Found average scores for: audio_wavonly_10s_9.5soverlap_2025_06_13_232310\n",
      "✓ Found average scores for: audio_full_10s_9.5soverlap_2025_06_13_232517\n",
      "\n",
      "==================================================\n",
      "COMPARISON OF AVERAGE SCORES ACROSS EXPERIMENTS\n",
      "==================================================\n",
      "\n",
      "KEY PERFORMANCE METRICS:\n",
      "------------------------------\n",
      "               experiment  test_loss\n",
      "audio_wavonly_10s_9.5s...     3.7082\n",
      "audio_full_10s_9.5sove...     2.8825\n",
      "\n",
      "\n",
      "ADDITIONAL METRICS:\n",
      "--------------------\n",
      "               experiment  shift_hold  short_long  shift_pred  bc_pred\n",
      "audio_wavonly_10s_9.5s...      0.9313      0.7748      0.5546   0.5683\n",
      "audio_full_10s_9.5sove...      0.9382      0.8158      0.6890   0.6971\n",
      "\n",
      "\n",
      "BEST PERFORMING EXPERIMENTS:\n",
      "-----------------------------------\n",
      "Lowest Test Loss    : audio_full_10s_9.5soverlap_2025_06_13_232517 (2.8825)\n",
      "Highest Shift F1    : audio_full_10s_9.5soverlap_2025_06_13_232517 (0.2140)\n",
      "Highest Accuracy    : audio_full_10s_9.5soverlap_2025_06_13_232517 (0.3593)\n",
      "Highest Hold F1     : audio_wavonly_10s_9.5soverlap_2025_06_13_232310 (0.9762)\n",
      "\n",
      "\n",
      "Full DataFrame shape: (2, 25)\n",
      "All columns: ['experiment', 'model', 'score_json_path', 'test_loss', 'shift_hold', 'short_long', 'short_long_0', 'short_long_1', 'shift_pred', 'shift_pred_0', 'shift_pred_1', 'ov_pred', 'ov_pred_0', 'ov_pred_1', 'bc_pred', 'bc_pred_0', 'bc_pred_1', 'shift_f1', 'shift_precision', 'shift_recall', 'hold_f1', 'hold_precision', 'hold_recall', 'accuracy', 'top_k_accuracy']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "csv_dir = '/home/eeyifanshen/e2e_audio_LLM/multi_modal_vap/output'\n",
    "\n",
    "def read_all_final_scores(output_dir):\n",
    "    \"\"\"Read all final_score.csv files and extract average rows for comparison\"\"\"\n",
    "    all_averages = []\n",
    "    \n",
    "    # Find all directories containing final_score.csv\n",
    "    for folder in os.listdir(output_dir):\n",
    "        folder_path = os.path.join(output_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            csv_path = os.path.join(folder_path, 'final_score.csv')\n",
    "            if os.path.exists(csv_path):\n",
    "                try:\n",
    "                    # Read the CSV file\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    \n",
    "                    # Find the average row\n",
    "                    avg_row = df[df['model'] == 'Average'].copy()\n",
    "                    if not avg_row.empty:\n",
    "                        # Add experiment name from folder\n",
    "                        avg_row['experiment'] = folder\n",
    "                        all_averages.append(avg_row)\n",
    "                        print(f\"✓ Found average scores for: {folder}\")\n",
    "                    else:\n",
    "                        print(f\"⚠ No 'Average' row found in: {folder}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error reading {csv_path}: {e}\")\n",
    "    \n",
    "    if not all_averages:\n",
    "        print(\"No valid final_score.csv files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all average rows\n",
    "    combined_df = pd.concat(all_averages, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns to put experiment first\n",
    "    cols = ['experiment'] + [col for col in combined_df.columns if col != 'experiment']\n",
    "    combined_df = combined_df[cols]\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Read all final scores\n",
    "print(\"Scanning for final_score.csv files...\")\n",
    "print(\"=\" * 50)\n",
    "results_df = read_all_final_scores(csv_dir)\n",
    "\n",
    "if results_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COMPARISON OF AVERAGE SCORES ACROSS EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display key metrics in a clean format\n",
    "    key_metrics = ['experiment', 'test_loss', ]\n",
    "    #  'shift_f1', 'shift_precision', 'shift_recall', \n",
    "                #    'hold_f1', 'hold_precision', 'hold_recall', 'accuracy', 'top_k_accuracy',]\n",
    "    \n",
    "    if all(col in results_df.columns for col in key_metrics):\n",
    "        print(\"\\nKEY PERFORMANCE METRICS:\")\n",
    "        print(\"-\" * 30)\n",
    "        display_df = results_df[key_metrics].round(4)\n",
    "        print(display_df.to_string(index=False, max_colwidth=25))\n",
    "    \n",
    "    # # Display additional metrics\n",
    "    # additional_metrics = ['shift_hold', 'short_long', 'shift_pred', 'ov_pred', 'bc_pred']\n",
    "    additional_metrics = ['shift_hold', 'short_long', 'shift_pred', 'bc_pred',]\n",
    "    \n",
    "    available_additional = [col for col in additional_metrics if col in results_df.columns]\n",
    "    \n",
    "    if available_additional:\n",
    "        print(f\"\\n\\nADDITIONAL METRICS:\")\n",
    "        print(\"-\" * 20)\n",
    "        additional_df = results_df[['experiment'] + available_additional].round(4)\n",
    "        print(additional_df.to_string(index=False, max_colwidth=25))\n",
    "    \n",
    "    # Show best performing experiment for key metrics\n",
    "    print(f\"\\n\\nBEST PERFORMING EXPERIMENTS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    metrics_to_check = {\n",
    "        'Lowest Test Loss': ('test_loss', 'min'),\n",
    "        'Highest Shift F1': ('shift_f1', 'max'),\n",
    "        'Highest Accuracy': ('accuracy', 'max'),\n",
    "        'Highest Hold F1': ('hold_f1', 'max')\n",
    "    }\n",
    "    \n",
    "    for metric_name, (column, operation) in metrics_to_check.items():\n",
    "        if column in results_df.columns:\n",
    "            if operation == 'min':\n",
    "                best_idx = results_df[column].idxmin()\n",
    "            else:\n",
    "                best_idx = results_df[column].idxmax()\n",
    "            \n",
    "            best_exp = results_df.loc[best_idx, 'experiment']\n",
    "            best_value = results_df.loc[best_idx, column]\n",
    "            print(f\"{metric_name:20}: {best_exp} ({best_value:.4f})\")\n",
    "    \n",
    "    print(f\"\\n\\nFull DataFrame shape: {results_df.shape}\")\n",
    "    print(\"All columns:\", list(results_df.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_modal_vap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
