{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c686b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for final_score.csv files...\n",
      "==================================================\n",
      "✓ Found average scores for: audio_wav_vis_null_2025_06_11_143013\n",
      "✓ Found average scores for: audio_wav_va_vis_null_2025_06_11_143157\n",
      "✓ Found average scores for: audio_full_vis_null_2025_06_11_143327\n",
      "✓ Found average scores for: audio_null_vis_gaze_2025_06_11_143518\n",
      "✓ Found average scores for: audio_null_vis_au_2025_06_11_143655\n",
      "✓ Found average scores for: audio_null_vis_head_2025_06_11_143810\n",
      "✓ Found average scores for: audio_null_vis_pose_2025_06_11_143955\n",
      "✓ Found average scores for: audio_null_vis_full_2025_06_11_144146\n",
      "✓ Found average scores for: audio_full_vis_gaze_2025_06_11_144446\n",
      "✓ Found average scores for: audio_full_vis_au_2025_06_11_144658\n",
      "✓ Found average scores for: audio_full_vis_head_2025_06_11_144859\n",
      "✓ Found average scores for: audio_full_vis_pose_2025_06_11_145135\n",
      "✓ Found average scores for: audio_full_vis_full_2025_06_11_145413\n",
      "\n",
      "==================================================\n",
      "COMPARISON OF AVERAGE SCORES ACROSS EXPERIMENTS\n",
      "==================================================\n",
      "\n",
      "KEY PERFORMANCE METRICS:\n",
      "------------------------------\n",
      "               experiment  test_loss\n",
      "audio_wav_vis_null_202...     3.5005\n",
      "audio_wav_va_vis_null_...     2.9096\n",
      "audio_full_vis_null_20...     2.9005\n",
      "audio_null_vis_gaze_20...     3.5669\n",
      "audio_null_vis_au_2025...     3.6739\n",
      "audio_null_vis_head_20...     3.5762\n",
      "audio_null_vis_pose_20...     3.6386\n",
      "audio_null_vis_full_20...     3.6263\n",
      "audio_full_vis_gaze_20...     2.8722\n",
      "audio_full_vis_au_2025...     2.8961\n",
      "audio_full_vis_head_20...     2.8830\n",
      "audio_full_vis_pose_20...     2.8873\n",
      "audio_full_vis_full_20...     2.9128\n",
      "\n",
      "\n",
      "ADDITIONAL METRICS:\n",
      "--------------------\n",
      "               experiment  shift_hold  short_long  shift_pred  bc_pred\n",
      "audio_wav_vis_null_202...      0.9296      0.7710      0.6097   0.6872\n",
      "audio_wav_va_vis_null_...      0.9325      0.8619      0.6769   0.6712\n",
      "audio_full_vis_null_20...      0.9360      0.8391      0.7118   0.6939\n",
      "audio_null_vis_gaze_20...      0.9281      0.7758      0.6402   0.6961\n",
      "audio_null_vis_au_2025...      0.9288      0.7734      0.6489   0.6577\n",
      "audio_null_vis_head_20...      0.9297      0.7679      0.6410   0.8071\n",
      "audio_null_vis_pose_20...      0.9291      0.7605      0.6456   0.8003\n",
      "audio_null_vis_full_20...      0.9292      0.7725      0.6325   0.6655\n",
      "audio_full_vis_gaze_20...      0.9378      0.8539      0.7106   0.6896\n",
      "audio_full_vis_au_2025...      0.9381      0.8383      0.7301   0.6768\n",
      "audio_full_vis_head_20...      0.9379      0.8512      0.7146   0.7215\n",
      "audio_full_vis_pose_20...      0.9384      0.8324      0.7170   0.6829\n",
      "audio_full_vis_full_20...      0.9364      0.8424      0.7269   0.6795\n",
      "\n",
      "\n",
      "BEST PERFORMING EXPERIMENTS:\n",
      "-----------------------------------\n",
      "Lowest Test Loss    : audio_full_vis_gaze_2025_06_11_144446 (2.8722)\n",
      "Highest Shift F1    : audio_full_vis_au_2025_06_11_144658 (0.2654)\n",
      "Highest Accuracy    : audio_full_vis_gaze_2025_06_11_144446 (0.3642)\n",
      "Highest Hold F1     : audio_null_vis_head_2025_06_11_143810 (0.9757)\n",
      "\n",
      "\n",
      "Full DataFrame shape: (13, 25)\n",
      "All columns: ['experiment', 'model', 'score_json_path', 'test_loss', 'shift_hold', 'short_long', 'short_long_0', 'short_long_1', 'shift_pred', 'shift_pred_0', 'shift_pred_1', 'ov_pred', 'ov_pred_0', 'ov_pred_1', 'bc_pred', 'bc_pred_0', 'bc_pred_1', 'shift_f1', 'shift_precision', 'shift_recall', 'hold_f1', 'hold_precision', 'hold_recall', 'accuracy', 'top_k_accuracy']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "csv_dir = '/home/eeyifanshen/e2e_audio_LLM/multi_modal_vap/output'\n",
    "\n",
    "def read_all_final_scores(output_dir):\n",
    "    \"\"\"Read all final_score.csv files and extract average rows for comparison\"\"\"\n",
    "    all_averages = []\n",
    "    \n",
    "    # Find all directories containing final_score.csv\n",
    "    for folder in os.listdir(output_dir):\n",
    "        folder_path = os.path.join(output_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            csv_path = os.path.join(folder_path, 'final_score.csv')\n",
    "            if os.path.exists(csv_path):\n",
    "                try:\n",
    "                    # Read the CSV file\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    \n",
    "                    # Find the average row\n",
    "                    avg_row = df[df['model'] == 'Average'].copy()\n",
    "                    if not avg_row.empty:\n",
    "                        # Add experiment name from folder\n",
    "                        avg_row['experiment'] = folder\n",
    "                        all_averages.append(avg_row)\n",
    "                        print(f\"✓ Found average scores for: {folder}\")\n",
    "                    else:\n",
    "                        print(f\"⚠ No 'Average' row found in: {folder}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error reading {csv_path}: {e}\")\n",
    "    \n",
    "    if not all_averages:\n",
    "        print(\"No valid final_score.csv files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all average rows\n",
    "    combined_df = pd.concat(all_averages, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns to put experiment first\n",
    "    cols = ['experiment'] + [col for col in combined_df.columns if col != 'experiment']\n",
    "    combined_df = combined_df[cols]\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Read all final scores\n",
    "print(\"Scanning for final_score.csv files...\")\n",
    "print(\"=\" * 50)\n",
    "results_df = read_all_final_scores(csv_dir)\n",
    "\n",
    "if results_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COMPARISON OF AVERAGE SCORES ACROSS EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display key metrics in a clean format\n",
    "    key_metrics = ['experiment', 'test_loss']\n",
    "    #  'shift_f1', 'shift_precision', 'shift_recall', \n",
    "                #    'hold_f1', 'hold_precision', 'hold_recall', 'accuracy', 'top_k_accuracy',]\n",
    "    \n",
    "    if all(col in results_df.columns for col in key_metrics):\n",
    "        print(\"\\nKEY PERFORMANCE METRICS:\")\n",
    "        print(\"-\" * 30)\n",
    "        display_df = results_df[key_metrics].round(4)\n",
    "        print(display_df.to_string(index=False, max_colwidth=25))\n",
    "    \n",
    "    # # Display additional metrics\n",
    "    # additional_metrics = ['shift_hold', 'short_long', 'shift_pred', 'ov_pred', 'bc_pred']\n",
    "    additional_metrics = ['shift_hold', 'short_long', 'shift_pred', 'bc_pred']\n",
    "    \n",
    "    available_additional = [col for col in additional_metrics if col in results_df.columns]\n",
    "    \n",
    "    if available_additional:\n",
    "        print(f\"\\n\\nADDITIONAL METRICS:\")\n",
    "        print(\"-\" * 20)\n",
    "        additional_df = results_df[['experiment'] + available_additional].round(4)\n",
    "        print(additional_df.to_string(index=False, max_colwidth=25))\n",
    "    \n",
    "    # Show best performing experiment for key metrics\n",
    "    print(f\"\\n\\nBEST PERFORMING EXPERIMENTS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    metrics_to_check = {\n",
    "        'Lowest Test Loss': ('test_loss', 'min'),\n",
    "        'Highest Shift F1': ('shift_f1', 'max'),\n",
    "        'Highest Accuracy': ('accuracy', 'max'),\n",
    "        'Highest Hold F1': ('hold_f1', 'max')\n",
    "    }\n",
    "    \n",
    "    for metric_name, (column, operation) in metrics_to_check.items():\n",
    "        if column in results_df.columns:\n",
    "            if operation == 'min':\n",
    "                best_idx = results_df[column].idxmin()\n",
    "            else:\n",
    "                best_idx = results_df[column].idxmax()\n",
    "            \n",
    "            best_exp = results_df.loc[best_idx, 'experiment']\n",
    "            best_value = results_df.loc[best_idx, column]\n",
    "            print(f\"{metric_name:20}: {best_exp} ({best_value:.4f})\")\n",
    "    \n",
    "    print(f\"\\n\\nFull DataFrame shape: {results_df.shape}\")\n",
    "    print(\"All columns:\", list(results_df.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_modal_vap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
